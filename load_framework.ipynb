{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sign-language-processing/datasets/blob/master/examples/load.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ov6fuFwGjlsy"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install git+https://github.com/sign-language-processing/datasets.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C4PZsi6pPp9j"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 15:47:30.393660: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-16 15:47:31.173351: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-16 15:47:31.174560: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-16 15:47:37.351767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import sign_language_datasets.datasets\n",
    "from sign_language_datasets.datasets.config import SignDatasetConfig\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKGZ4JXCZmSE"
   },
   "source": [
    "# RWTH Phoenix 2014 T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8wU1Q4URqRBE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Using custom data configuration rwth_phoenix2014_t_poses\n",
      "2024-02-16 15:47:45.723105: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ./datasets/rwth_phoenix2014_t/rwth_phoenix2014_t_poses/3.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedroodb/.local/lib/python3.8/site-packages/sign_language_datasets/datasets/warning.py:5: UserWarning: This library provides access to data sets without claiming ownership over them or defining their licensing terms. Users who download data are responsible for checking the license of each individual data set.\n",
      "  warnings.warn(\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 43.36 url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 32.42 url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 27.50 url/s]\n",
      "Dl Completed...:  50%|█████     | 1/2 [00:00<00:00, 22.27 url/s]\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 38.74 url/s]\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 34.70 url/s]\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 32.69 url/s]\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\n",
      "Dl Size...: 100%|██████████| 5519547456/5519547456 [00:00<00:00, 68741808297.15 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 22.49 url/s]\n",
      "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]WARNING:absl:You use TensorFlow DType <dtype: 'float32'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to float32.\n",
      "WARNING:absl:You use TensorFlow DType <dtype: 'int32'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int32.\n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_json_content {'shape': [None, 1, 543, 3], 'encoding_format': 'pose', 'include_path': False}\n",
      "Dataset rwth_phoenix2014_t downloaded and prepared to ./datasets/rwth_phoenix2014_t/rwth_phoenix2014_t_poses/3.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "config = SignDatasetConfig(\n",
    "    name=\"rwth_phoenix2014_t_poses\",\n",
    "    version=\"3.0.0\",\n",
    "    include_video=False,\n",
    "    include_pose=\"holistic\",\n",
    ")\n",
    "rwth_phoenix2014_t = tfds.load(\n",
    "    name=\"rwth_phoenix2014_t\", builder_kwargs=dict(config=config), data_dir=DATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['gloss', 'id', 'pose', 'signer', 'text'])\n",
      "(47, 1, 543, 3)\n",
      "GUT ABEND LIEB ZUSCHAUER\n",
      "guten abend liebe zuschauer\n",
      "\n",
      "dict_keys(['gloss', 'id', 'pose', 'signer', 'text'])\n",
      "(56, 1, 543, 3)\n",
      "BERG DAZU SCHNEE\n",
      "im bergland fällt zunehmend schnee\n",
      "\n",
      "dict_keys(['gloss', 'id', 'pose', 'signer', 'text'])\n",
      "(70, 1, 543, 3)\n",
      "UNWETTER WEHEN\n",
      "und der wind weht auch noch kräftig aus west bis nordwest\n",
      "\n",
      "dict_keys(['gloss', 'id', 'pose', 'signer', 'text'])\n",
      "(99, 1, 543, 3)\n",
      "WIE-AUSSEHEN IN-KOMMEND MONTAG BIS MITTWOCH WIE-IMMER\n",
      "die aussichten von montag bis mittwoch ändert sich das wetter kaum\n",
      "\n",
      "dict_keys(['gloss', 'id', 'pose', 'signer', 'text'])\n",
      "(123, 1, 543, 3)\n",
      "BERG HOCH KOENNEN QUELL WOLKE NAH REGION KOENNEN VERDICHTEN\n",
      "über dem bergland können sich einzelne quellwolken zeigen in küstennähe gibt es auch mal dichtere wolken\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for datum in itertools.islice(rwth_phoenix2014_t[\"train\"], 0, 5):\n",
    "    print((datum.keys()))\n",
    "    print(datum[\"pose\"][\"data\"].shape)\n",
    "    print(datum[\"gloss\"].numpy().decode(\"utf-8\"))\n",
    "    print(datum[\"text\"].numpy().decode(\"utf-8\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6iBwM9lTzS6"
   },
   "source": [
    "# Dicta Sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEjOCQ_VYeUG"
   },
   "source": [
    "First, we set up HamNoSys in Google Colab for it to be visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN7S2m-BYc4s"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "font_url = \"https://ctan.math.illinois.edu/fonts/hamnosys/HamNoSysUnicode.ttf\"\n",
    "\n",
    "# Define the custom CSS to inject\n",
    "custom_css = f\"\"\"\n",
    "@font-face {{\n",
    "    font-family: 'HamNoSysUnicode';\n",
    "    src: url('{font_url}') format('truetype');\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML with the custom CSS\n",
    "HTML(f\"<style>{custom_css}</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQWUAgpVT0bK"
   },
   "outputs": [],
   "source": [
    "config = SignDatasetConfig(\n",
    "    name=\"only-annotations\", version=\"1.0.0\", include_video=False, include_pose=None\n",
    ")\n",
    "dicta_sign = tfds.load(name=\"dicta_sign\", builder_kwargs={\"config\": config})\n",
    "\n",
    "for datum in itertools.islice(dicta_sign[\"train\"], 0, 10):\n",
    "    hamnosys_text = datum[\"hamnosys\"].numpy().decode(\"utf-8\")\n",
    "    plain_text = datum[\"text\"].numpy().decode(\"utf-8\")\n",
    "    display(\n",
    "        HTML(\n",
    "            f'<span style=\"font-family:HamNoSysUnicode;\">{hamnosys_text}</span> {plain_text}'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcIs13W6TfWz"
   },
   "source": [
    "# ChicagoFSWild+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1X1kIgoTfec"
   },
   "outputs": [],
   "source": [
    "# Version 2.0.0 is ChicagoFSWild+, 1.0.0 is ChicagoFSWild\n",
    "config = SignDatasetConfig(\n",
    "    name=\"only-annotations\", version=\"2.0.0\", include_video=False\n",
    ")\n",
    "chicagofswild = tfds.load(name=\"chicago_fs_wild\", builder_kwargs=dict(config=config))\n",
    "\n",
    "for datum in itertools.islice(chicagofswild[\"train\"], 0, 10):\n",
    "    print(datum[\"text\"].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XK7jyOOtYv_P"
   },
   "source": [
    "# AUTSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfZnI9K8YxfJ"
   },
   "outputs": [],
   "source": [
    "config = SignDatasetConfig(\n",
    "    name=\"only-annotations\", version=\"1.0.0\", include_video=False\n",
    ")\n",
    "autsl = tfds.load(name=\"autsl\", builder_kwargs={\"config\": config})\n",
    "\n",
    "for datum in itertools.islice(autsl[\"train\"], 0, 10):\n",
    "    print(datum[\"id\"].numpy().decode(\"utf-8\"), datum[\"gloss_id\"].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rykmI68x3E07"
   },
   "source": [
    "# SignBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12XcWfeg21kE"
   },
   "outputs": [],
   "source": [
    "signbank = tfds.load(name=\"sign_bank\")\n",
    "\n",
    "for datum in itertools.islice(signbank[\"train\"], 0, 10):\n",
    "    print(\n",
    "        datum[\"id\"].numpy().decode(\"utf-8\"),\n",
    "        datum[\"sign_writing\"].numpy().decode(\"utf-8\"),\n",
    "        [f.decode(\"utf-8\") for f in datum[\"terms\"].numpy()],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biXjC80j17n1"
   },
   "source": [
    "# SignTyp (https://signtyp.uconn.edu/signpuddle/index.php?ui=1&sgn=9032)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVgbyUIg165c"
   },
   "outputs": [],
   "source": [
    "config = SignDatasetConfig(\n",
    "    name=\"only-annotations\",\n",
    "    version=\"1.0.0\",\n",
    "    include_video=False,\n",
    "    extra={\"PHPSESSID\": \"hj9co07ct7f5noq529no9u09l4\"},\n",
    ")\n",
    "signtyp = tfds.load(name=\"sign_typ\", builder_kwargs=dict(config=config))\n",
    "\n",
    "for datum in itertools.islice(signtyp[\"train\"], 0, 10):\n",
    "    print(\n",
    "        datum[\"video\"].numpy().decode(\"utf-8\"),\n",
    "        datum[\"sign_writing\"].numpy().decode(\"utf-8\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOLfw9-z2qK7"
   },
   "source": [
    "# Sign2Mint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X96ogmu_22zv"
   },
   "outputs": [],
   "source": [
    "config = SignDatasetConfig(\n",
    "    name=\"only-annotations\", version=\"1.0.0\", include_video=False\n",
    ")\n",
    "sign2mint = tfds.load(name=\"sign2_mint\", builder_kwargs={\"config\": config})\n",
    "\n",
    "for datum in itertools.islice(sign2mint[\"train\"], 0, 10):\n",
    "    print(\n",
    "        datum[\"fachbegriff\"].numpy().decode(\"utf-8\"),\n",
    "        datum[\"video\"].numpy().decode(\"utf-8\"),\n",
    "        datum[\"gebaerdenschrift\"][\"url\"].numpy().decode(\"utf-8\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnf4AaX936w4"
   },
   "source": [
    "# SWOJS Glossário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shQxQtQP359y"
   },
   "outputs": [],
   "source": [
    "config = SignDatasetConfig(\n",
    "    name=\"only-annotations\", version=\"1.0.0\", include_video=False\n",
    ")\n",
    "swojs_glossario = tfds.load(name=\"swojs_glossario\", builder_kwargs={\"config\": config})\n",
    "\n",
    "\n",
    "def decode(tl):\n",
    "    return list(map(lambda t: t.decode(\"utf-8\"), tl.numpy()))\n",
    "\n",
    "\n",
    "for datum in itertools.islice(swojs_glossario[\"train\"], 0, 10):\n",
    "    print(decode(datum[\"sign_writing\"]), datum[\"video\"].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNJdG7ExZugh"
   },
   "source": [
    "# DGS Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVjrhsbtbWbX"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install pympi-ling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "G1ZxOKNKX2Q3"
   },
   "source": [
    "## Document Level example (Long videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1XvMTs9Zvx4"
   },
   "outputs": [],
   "source": [
    "from sign_language_datasets.datasets.dgs_corpus import DgsCorpusConfig\n",
    "\n",
    "config = DgsCorpusConfig(\n",
    "    name=\"only-annotations\", version=\"1.0.0\", include_video=False, include_pose=None\n",
    ")\n",
    "dgs_corpus = tfds.load(\"dgs_corpus\", builder_kwargs=dict(config=config))\n",
    "\n",
    "from sign_language_datasets.datasets.dgs_corpus.dgs_utils import get_elan_sentences\n",
    "\n",
    "for datum in itertools.islice(dgs_corpus[\"train\"], 0, 10):\n",
    "    elan_path = datum[\"paths\"][\"eaf\"].numpy().decode(\"utf-8\")\n",
    "    sentences = get_elan_sentences(elan_path)\n",
    "\n",
    "    try:\n",
    "        sentence = next(sentences)\n",
    "        print(\" \".join([s[\"gloss\"] for s in sentence[\"glosses\"]]))\n",
    "        print(sentence[\"german\"])\n",
    "        print()\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "6gaTQgmQX2Q3"
   },
   "source": [
    "## Sentence level example (Videos are broken down to sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSWN-bpCX2Q3"
   },
   "outputs": [],
   "source": [
    "from sign_language_datasets.datasets.dgs_corpus import DgsCorpusConfig\n",
    "\n",
    "config = DgsCorpusConfig(\n",
    "    name=\"only-annotations-sentence-level\",\n",
    "    version=\"1.0.0\",\n",
    "    include_video=False,\n",
    "    include_pose=None,\n",
    "    data_type=\"sentence\",\n",
    ")\n",
    "dgs_corpus = tfds.load(\"dgs_corpus\", builder_kwargs=dict(config=config))\n",
    "\n",
    "for datum in itertools.islice(dgs_corpus[\"train\"], 0, 5):\n",
    "    sentence = datum[\"sentence\"][\"german\"].numpy().decode(\"utf-8\")\n",
    "    print(sentence)\n",
    "    print(datum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gyb2vSTLX2Q3"
   },
   "source": [
    "# DGS Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUbtkMMKX2Q3"
   },
   "outputs": [],
   "source": [
    "config = SignDatasetConfig(\n",
    "    name=\"only-annotations\",\n",
    "    version=\"1.0.0\",\n",
    "    include_video=False,\n",
    "    include_pose=None,\n",
    "    process_video=False,\n",
    ")\n",
    "dgs_types = tfds.load(\"dgs_types\", builder_kwargs=dict(config=config))\n",
    "\n",
    "for datum in itertools.islice(dgs_types[\"train\"], 0, 10):\n",
    "    print(datum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tS4GuMBbX2Q3"
   },
   "source": [
    "# Sign Suisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kti4k3L3X2Q3"
   },
   "outputs": [],
   "source": [
    "config = SignDatasetConfig(\n",
    "    name=\"only-annotations\", version=\"1.0.0\", include_video=False, process_video=False\n",
    ")\n",
    "sign_suisse = tfds.load(\"sign_suisse\", builder_kwargs=dict(config=config))\n",
    "\n",
    "for datum in itertools.islice(sign_suisse[\"train\"], 0, 10):\n",
    "    print(datum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "KmJtA1UUX2Q3"
   },
   "source": [
    "# NGT Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vayqFcrKX2Q4"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install pympi-ling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TALQxYmeX2Q4"
   },
   "outputs": [],
   "source": [
    "from sign_language_datasets.datasets.ngt_corpus.ngt_corpus_utils import (\n",
    "    get_elan_sentences_ngt_corpus,\n",
    ")\n",
    "\n",
    "config = SignDatasetConfig(\n",
    "    name=\"only-annotations\", version=\"1.0.0\", include_video=False\n",
    ")\n",
    "ngt = tfds.load(name=\"ngt_corpus\", builder_kwargs={\"config\": config})\n",
    "\n",
    "for datum in itertools.islice(ngt[\"train\"], 0, 10):\n",
    "    print(datum[\"id\"].numpy().decode(\"utf-8\"))\n",
    "    elan_path = datum[\"paths\"][\"eaf\"].numpy().decode(\"utf-8\")\n",
    "\n",
    "    sentences = get_elan_sentences_ngt_corpus(elan_path)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Lc6eoG7OX2Q4"
   },
   "source": [
    "# BSL Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdrAR3ooX2Q4"
   },
   "outputs": [],
   "source": [
    "from sign_language_datasets.datasets.bsl_corpus.bsl_corpus_utils import (\n",
    "    get_elan_sentences_bsl_corpus,\n",
    ")\n",
    "\n",
    "# this corpus requires a login\n",
    "\n",
    "BSLCP_USERNAME = \"\"\n",
    "BSLCP_PASSWORD = \"\"\n",
    "\n",
    "config = SignDatasetConfig(\n",
    "    name=\"only-annotations\", version=\"1.0.0\", include_video=False, include_pose=None\n",
    ")\n",
    "\n",
    "bslcp = tfds.load(\n",
    "    name=\"bsl_corpus\",\n",
    "    builder_kwargs={\n",
    "        \"config\": config,\n",
    "        \"bslcp_username\": BSLCP_USERNAME,\n",
    "        \"bslcp_password\": BSLCP_PASSWORD,\n",
    "    },\n",
    ")\n",
    "\n",
    "for datum in itertools.islice(bslcp[\"train\"], 0, 10):\n",
    "    print(datum[\"id\"].numpy().decode(\"utf-8\"))\n",
    "    elan_path = datum[\"paths\"][\"eaf\"][0].numpy().decode(\"utf-8\")\n",
    "\n",
    "    sentences = get_elan_sentences_bsl_corpus(elan_path)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "K984lKOuX2Q4"
   },
   "source": [
    "# WMT-SLT\n",
    "\n",
    "Instructions and example code are here: https://github.com/sign-language-processing/datasets/blob/master/sign_language_datasets/datasets/wmt_slt/README.md"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Sign Language Datasets Example",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
