{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not running on Google Colab\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "\tfrom google.colab import drive\n",
        "\n",
        "\tIN_COLAB = True\n",
        "\tprint(\"Running on Google Colab\")\n",
        "\tdrive.mount('/content/drive')\n",
        "except:\n",
        "\tIN_COLAB = False\n",
        "\tprint(\"Not running on Google Colab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U32NXn0wYoP6"
      },
      "source": [
        "## Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bR3ET68zYoP9"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "\t!pip install git+https://github.com/sign-language-processing/datasets.git -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zqqm0eD3cD-G"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "# import sign_language_datasets.datasets\n",
        "from sign_language_datasets.utils.torch_dataset import TFDSTorchDataset\n",
        "from sign_language_datasets.datasets.config import SignDatasetConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RcUF3vxZYoP_"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \".\" if not IN_COLAB else \"/content/drive/MyDrive/Académico/Doctorado/SLT Datasets/RWTH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i4O14ovVYoP_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Using custom data configuration rwth_phoenix2014_t_poses\n",
            "WARNING:absl:You use TensorFlow DType <dtype: 'float32'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to float32.\n",
            "WARNING:absl:You use TensorFlow DType <dtype: 'int32'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int32.\n"
          ]
        }
      ],
      "source": [
        "config = SignDatasetConfig(name=\"rwth_phoenix2014_t_poses\", version=\"3.0.0\", include_video=False, include_pose=\"holistic\")\n",
        "rwth_phoenix2014_t = tfds.load(name='rwth_phoenix2014_t', builder_kwargs=dict(config=config), data_dir=DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8BAXZsLtYoQA"
      },
      "outputs": [],
      "source": [
        "train_dataset = TFDSTorchDataset(rwth_phoenix2014_t[\"train\"])\n",
        "test_dataset = TFDSTorchDataset(rwth_phoenix2014_t[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c1d4PlXWYoQA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['gloss', 'id', 'pose', 'signer', 'text'])\n",
            "Pose shape: torch.Size([47, 1, 543, 3])\n",
            "Text: guten abend liebe zuschauer\n",
            "\n",
            "dict_keys(['gloss', 'id', 'pose', 'signer', 'text'])\n",
            "Pose shape: torch.Size([56, 1, 543, 3])\n",
            "Text: im bergland fällt zunehmend schnee\n",
            "\n",
            "dict_keys(['gloss', 'id', 'pose', 'signer', 'text'])\n",
            "Pose shape: torch.Size([70, 1, 543, 3])\n",
            "Text: und der wind weht auch noch kräftig aus west bis nordwest\n",
            "\n",
            "dict_keys(['gloss', 'id', 'pose', 'signer', 'text'])\n",
            "Pose shape: torch.Size([99, 1, 543, 3])\n",
            "Text: die aussichten von montag bis mittwoch ändert sich das wetter kaum\n",
            "\n",
            "dict_keys(['gloss', 'id', 'pose', 'signer', 'text'])\n",
            "Pose shape: torch.Size([123, 1, 543, 3])\n",
            "Text: über dem bergland können sich einzelne quellwolken zeigen in küstennähe gibt es auch mal dichtere wolken\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# import itertools\n",
        "\n",
        "\n",
        "# for datum in itertools.islice(train_dataset, 0, 5):\n",
        "# \tprint((datum.keys()))\n",
        "# \tprint(f\"Pose shape: {datum['pose']['data'].shape}\")\n",
        "# \tprint(f\"Text: {datum['text'].decode('utf-8')}\")\n",
        "# \tprint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-K1iWHgYoQB"
      },
      "source": [
        "## Dataset analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vM_qq128YoQB"
      },
      "outputs": [],
      "source": [
        "# src_lenghts = []\n",
        "# texts = []\n",
        "\n",
        "# for datum in rwth_phoenix2014_t[\"train\"]:\n",
        "# \tsrc_lenghts.append(datum['pose']['data'].shape[0])\n",
        "# \ttexts.append(datum['text'].numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK5Y_TkwYoQB"
      },
      "source": [
        "### Frames analysis for padding and truncation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ltZo3Sw5YoQC"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# src_lengths_df = pd.Series(src_lenghts)\n",
        "# src_lengths_df.describe(percentiles=[.75, .9, .95, .99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tQej-Lo5YoQC"
      },
      "outputs": [],
      "source": [
        "# src_lengths_df.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48qWRik4YoQC"
      },
      "source": [
        "### Text tokenization and analysis for padding and truncation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E7M8enEwYoQC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "TEXT_MODEL = \"google-bert/bert-base-german-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7LY_zdI9RaX1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOS_IDX: 3, EOS_IDX: 4, PAD_IDX: 0\n"
          ]
        }
      ],
      "source": [
        "BOS_IDX = tokenizer.cls_token_id if tokenizer.cls_token_id is not None else -1\n",
        "EOS_IDX = tokenizer.sep_token_id if tokenizer.sep_token_id is not None else -1\n",
        "PAD_IDX = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else -1\n",
        "\n",
        "print(f\"BOS_IDX: {BOS_IDX}, EOS_IDX: {EOS_IDX}, PAD_IDX: {PAD_IDX}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V_OQsn4lYoQC"
      },
      "outputs": [],
      "source": [
        "# tokenized_sequences = tokenizer(texts, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kSCWs0VOYoQC"
      },
      "outputs": [],
      "source": [
        "# tokens_length = [len(tokens) for tokens in tokenized_sequences['input_ids']]\n",
        "# print(max(tokens_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2hvQBgYrYoQD"
      },
      "outputs": [],
      "source": [
        "# print(texts[0])\n",
        "# print(tokenized_sequences[0].ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLJDnSFUYoQD"
      },
      "source": [
        "## Preprocessing and dataloader generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_FRAMES = 259\n",
        "MAX_TOKENS = 80\n",
        "BATCH_SIZE = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fomPDN0QYoQD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.utils.data as utils\n",
        "\n",
        "\n",
        "def flatten_keypoints(datum: Tensor):\n",
        "\t'''\n",
        "\t\tReshape the pose of datum only keeping the first dimension S (sequence lenght) and flattening the number of keypoints K and their dimensions D.\n",
        "\t\tArgs:\n",
        "\t\t\tdatum: Tensor of shape (S, D, K)\n",
        "\t\tReturns:\n",
        "\t\t\tTensor of shape (frames, D * K)\n",
        "\t'''\n",
        "\treturn datum.view(datum.size(0), -1)\n",
        "\n",
        "def pad_truncate_src(datum: Tensor, max_len: int):\n",
        "\t'''Pad the pose to max_len or truncate it'''\n",
        "\tif datum.size(0) < max_len:\n",
        "\t\treturn torch.cat([datum, torch.zeros(max_len - datum.size(0), datum.size(1))])\n",
        "\telse:\n",
        "\t\treturn datum[:max_len]\n",
        "\n",
        "def collate_fn(batch):\n",
        "\tsrc = [item['pose']['data'] for item in batch]\n",
        "\tsrc = torch.stack([pad_truncate_src(flatten_keypoints(datum), MAX_FRAMES) for datum in src])\n",
        "\ttgt = [str(item['text'].decode('utf-8')) for item in batch]\n",
        "\ttgt = tokenizer(tgt, padding='max_length', max_length=MAX_TOKENS, return_tensors='pt').input_ids\n",
        "\treturn src, tgt\n",
        "\n",
        "train_loader = utils.DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "test_loader = utils.DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ueFV0vG-h4ze"
      },
      "outputs": [],
      "source": [
        "# for src, tgt in train_loader:\n",
        "#   print(src.shape)\n",
        "#   print(tgt.shape)\n",
        "#   break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAr2B2UWYoQD"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDR0rGzsYoQD"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cRabvZjw3_HI"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "\t!pip install lightning -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jsbnlFGSYoQD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(size: int, device: torch.device):\n",
        "    '''\n",
        "        Generates triangular (size, size) mask for the transformer model.\n",
        "    '''\n",
        "    mask = (torch.triu(torch.ones((size, size))) == 1).transpose(0, 1).to(device)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_target_mask(tgt: Tensor, pad_idx: int, device: torch.device):\n",
        "    '''\n",
        "        Create target mask and padding mask for the transformer model.\n",
        "        Args:\n",
        "            tgt: (N, T) where N is the batch size and T is the target sequence length\n",
        "            pad_idx: padding index\n",
        "            device: torch device\n",
        "        Returns:\n",
        "            tgt_mask: (T, T), so to evaluate the i-th token, we can only look at the first i tokens, for all i's\n",
        "            tgt_padding_mask: (N, T), for masking pad tokens\n",
        "    '''\n",
        "    tgt_seq_len = tgt.shape[1]\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)\n",
        "    tgt_padding_mask = (tgt == pad_idx)\n",
        "    return tgt_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import Tensor, nn\n",
        "from torch.nn.functional import relu\n",
        "\n",
        "\n",
        "class Conv1DEmbedder(nn.Module):\n",
        "\n",
        "\tdef __init__(self, in_channels: int, out_channels: int):\n",
        "\t\tsuper(Conv1DEmbedder, self).__init__()\n",
        "\t\tself.conv1d_1 = nn.Conv1d(in_channels, 512, 1)\n",
        "\t\tself.conv1d_2 = nn.Conv1d(512, 256, 1)\n",
        "\t\tself.conv1d_3 = nn.Conv1d(256, 128, 1)\n",
        "\t\tself.conv1d_4 = nn.Conv1d(128, out_channels, 1)\n",
        "\n",
        "\tdef forward(self, x: Tensor) -> Tensor:\n",
        "\t\t'''\n",
        "\t\t\tArgs:\n",
        "\t\t\t\tx: (N, S, E) where N is the batch size, S is the sequence length and E is the embedding size\n",
        "\t\t\tReturns:\n",
        "\t\t\t\t(N, S, E) where E is the embedding size\n",
        "\t\t'''\n",
        "\t\tx = x.permute(0, 2, 1)\n",
        "\t\tx = relu(self.conv1d_1(x))\n",
        "\t\tx = relu(self.conv1d_2(x))\n",
        "\t\tx = relu(self.conv1d_3(x))\n",
        "\t\tx = relu(self.conv1d_4(x))\n",
        "\t\treturn x.permute(0, 2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KK31oNMxYoQD"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        '''\n",
        "        Apply positional encoding to the input tensor.\n",
        "        Args:\n",
        "            x: (N, S, E)\n",
        "        Returns:\n",
        "            Tensor of shape (N, S, E)\n",
        "        '''\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AfZ_bIORYoQE"
      },
      "outputs": [],
      "source": [
        "# import math\n",
        "# from torch import nn, Tensor\n",
        "\n",
        "\n",
        "# class TokenEmbedding(nn.Module):\n",
        "#     '''Code taken from https://pytorch.org/tutorials/beginner/translation_transformer.html'''\n",
        "\n",
        "#     def __init__(self, vocab_size: int, emb_size):\n",
        "#         super(TokenEmbedding, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "#         self.emb_size = emb_size\n",
        "\n",
        "#     def forward(self, tokens: Tensor):\n",
        "#         '''\n",
        "#             Applies token embedding to the target tensor.\n",
        "#             Args:\n",
        "#                 tokens: (N, T)\n",
        "#             Returns:\n",
        "#                 Tensor of shape (N, T, E)\n",
        "#         '''\n",
        "#         return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4Ktl6PWcYoQE"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor, nn\n",
        "from torch import nn, Tensor\n",
        "from transformers import AutoModel\n",
        "\n",
        "\n",
        "class KeypointsTransformer(nn.Module):\n",
        "    '''\n",
        "        Transformer model for sign language translation. It uses a 1D convolutional layer to embed the keypoints and a transformer to translate the sequence.\n",
        "        S refers to the source sequence length, T to the target sequence length, N to the batch size, and E is the features number.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                src_max_len: int,\n",
        "                tgt_max_len: int,\n",
        "                in_features: int,\n",
        "                tgt_vocab_size: int,\n",
        "                d_model: int = 64,\n",
        "                dropout: float = 0.1\n",
        "                ):\n",
        "        '''\n",
        "            Args:\n",
        "                src_max_len: max length of the source sequence\n",
        "                tgt_max_len: max length of the target sequence\n",
        "                in_features: number of features of the input (amount of keypoints * amount of coordinates)\n",
        "                tgt_vocab_size: size of the target vocabulary\n",
        "                d_model: number of dimensions of the encoding vectors (default=64). Must be even so the positional encoding works.\n",
        "                kernel_size: the size of the 1D convolution window (default=5)\n",
        "                keys_initial_emb_size: the size of the keys embedding (default=128)\n",
        "        '''\n",
        "        super(KeypointsTransformer, self).__init__()\n",
        "\n",
        "        self.src_keyp_emb = Conv1DEmbedder(in_channels=in_features, out_channels=d_model)\n",
        "        self.src_pe = PositionalEncoding(d_model=d_model, max_len=src_max_len)\n",
        "        self.bert = AutoModel.from_pretrained(TEXT_MODEL)\n",
        "        self.bert.requires_grad_(False)\n",
        "        self.tgt_tok_emb = Conv1DEmbedder(in_channels=768, out_channels=d_model)\n",
        "        self.tgt_pe = PositionalEncoding(d_model=d_model, max_len=tgt_max_len)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, dropout=dropout, batch_first=True)\n",
        "        self.generator = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                tgt: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor\n",
        "    ):\n",
        "        '''\n",
        "            Forward pass of the model.\n",
        "            Args:\n",
        "                src: (N, S, E)\n",
        "                tgt: (N, T, E)\n",
        "                tgt_mask: (T, T)\n",
        "                tgt_padding_mask: (N, T)\n",
        "            Returns:\n",
        "                Tensor of shape (N, T, tgt_vocab_size)\n",
        "        '''\n",
        "        src_emb = self.src_keyp_emb(src)\n",
        "        src_emb = self.src_pe(src_emb)\n",
        "        tgt_emb = self.tgt_tok_emb(self.bert(tgt, attention_mask=(tgt == PAD_IDX)).last_hidden_state)\n",
        "        tgt_emb = self.tgt_pe(tgt_emb)\n",
        "        # src_mask and src_key_padding_mask are set to none as we use the whole input at every timestep\n",
        "        outs = self.transformer(\n",
        "            src = src_emb,\n",
        "            tgt = tgt_emb,\n",
        "            src_mask = None,\n",
        "            tgt_mask = tgt_mask,\n",
        "            src_key_padding_mask = None,\n",
        "            tgt_key_padding_mask = tgt_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor):\n",
        "        src_emb = self.src_pe(self.keys_emb(src))\n",
        "        return self.transformer.encoder(src_emb, None)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.tgt_pe(self.tgt_tok_emb(tgt)), memory, tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "D_MODEL = 64\n",
        "DROPOUT = 0.1\n",
        "\n",
        "NUM_KEYPOINTS = 543\n",
        "IN_FEATURES = NUM_KEYPOINTS*3\n",
        "\n",
        "model = KeypointsTransformer(\n",
        "    src_max_len=MAX_FRAMES,\n",
        "    tgt_max_len=MAX_TOKENS,\n",
        "    in_features=IN_FEATURES,\n",
        "    tgt_vocab_size=tokenizer.vocab_size,\n",
        "    d_model=D_MODEL,\n",
        "    dropout=DROPOUT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NtRybihvA-cu"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "\t!pip install modelsummary -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "o4OiNKV838b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 259, 1629]) torch.Size([1, 150]) torch.Size([150, 150]) torch.Size([1, 150])\n",
            "-----------------------------------------------------------------------\n",
            "             Layer (type)                Input Shape         Param #\n",
            "=======================================================================\n",
            "         Conv1DEmbedder-1            [-1, 259, 1629]               0\n",
            "                 Conv1d-2            [-1, 1629, 259]         834,560\n",
            "                 Conv1d-3             [-1, 512, 259]         131,328\n",
            "                 Conv1d-4             [-1, 256, 259]          32,896\n",
            "                 Conv1d-5             [-1, 128, 259]           8,256\n",
            "     PositionalEncoding-6              [-1, 259, 64]               0\n",
            "                Dropout-7              [-1, 259, 64]               0\n",
            "              BertModel-8                  [-1, 150]               0\n",
            "         BertEmbeddings-9                         ()               0\n",
            "             Embedding-10                  [-1, 150]      23,040,000\n",
            "             Embedding-11                  [-1, 150]           1,536\n",
            "             Embedding-12                  [-1, 150]         393,216\n",
            "             LayerNorm-13             [-1, 150, 768]           1,536\n",
            "               Dropout-14             [-1, 150, 768]               0\n",
            "           BertEncoder-15             [-1, 150, 768]               0\n",
            "             BertLayer-16             [-1, 150, 768]               0\n",
            "         BertAttention-17             [-1, 150, 768]               0\n",
            "     BertSelfAttention-18             [-1, 150, 768]               0\n",
            "                Linear-19             [-1, 150, 768]         590,592\n",
            "                Linear-20             [-1, 150, 768]         590,592\n",
            "                Linear-21             [-1, 150, 768]         590,592\n",
            "               Dropout-22         [-1, 12, 150, 150]               0\n",
            "        BertSelfOutput-23             [-1, 150, 768]               0\n",
            "                Linear-24             [-1, 150, 768]         590,592\n",
            "               Dropout-25             [-1, 150, 768]               0\n",
            "             LayerNorm-26             [-1, 150, 768]           1,536\n",
            "      BertIntermediate-27             [-1, 150, 768]               0\n",
            "                Linear-28             [-1, 150, 768]       2,362,368\n",
            "        GELUActivation-29            [-1, 150, 3072]               0\n",
            "            BertOutput-30            [-1, 150, 3072]               0\n",
            "                Linear-31            [-1, 150, 3072]       2,360,064\n",
            "               Dropout-32             [-1, 150, 768]               0\n",
            "             LayerNorm-33             [-1, 150, 768]           1,536\n",
            "             BertLayer-34             [-1, 150, 768]               0\n",
            "         BertAttention-35             [-1, 150, 768]               0\n",
            "     BertSelfAttention-36             [-1, 150, 768]               0\n",
            "                Linear-37             [-1, 150, 768]         590,592\n",
            "                Linear-38             [-1, 150, 768]         590,592\n",
            "                Linear-39             [-1, 150, 768]         590,592\n",
            "               Dropout-40         [-1, 12, 150, 150]               0\n",
            "        BertSelfOutput-41             [-1, 150, 768]               0\n",
            "                Linear-42             [-1, 150, 768]         590,592\n",
            "               Dropout-43             [-1, 150, 768]               0\n",
            "             LayerNorm-44             [-1, 150, 768]           1,536\n",
            "      BertIntermediate-45             [-1, 150, 768]               0\n",
            "                Linear-46             [-1, 150, 768]       2,362,368\n",
            "        GELUActivation-47            [-1, 150, 3072]               0\n",
            "            BertOutput-48            [-1, 150, 3072]               0\n",
            "                Linear-49            [-1, 150, 3072]       2,360,064\n",
            "               Dropout-50             [-1, 150, 768]               0\n",
            "             LayerNorm-51             [-1, 150, 768]           1,536\n",
            "             BertLayer-52             [-1, 150, 768]               0\n",
            "         BertAttention-53             [-1, 150, 768]               0\n",
            "     BertSelfAttention-54             [-1, 150, 768]               0\n",
            "                Linear-55             [-1, 150, 768]         590,592\n",
            "                Linear-56             [-1, 150, 768]         590,592\n",
            "                Linear-57             [-1, 150, 768]         590,592\n",
            "               Dropout-58         [-1, 12, 150, 150]               0\n",
            "        BertSelfOutput-59             [-1, 150, 768]               0\n",
            "                Linear-60             [-1, 150, 768]         590,592\n",
            "               Dropout-61             [-1, 150, 768]               0\n",
            "             LayerNorm-62             [-1, 150, 768]           1,536\n",
            "      BertIntermediate-63             [-1, 150, 768]               0\n",
            "                Linear-64             [-1, 150, 768]       2,362,368\n",
            "        GELUActivation-65            [-1, 150, 3072]               0\n",
            "            BertOutput-66            [-1, 150, 3072]               0\n",
            "                Linear-67            [-1, 150, 3072]       2,360,064\n",
            "               Dropout-68             [-1, 150, 768]               0\n",
            "             LayerNorm-69             [-1, 150, 768]           1,536\n",
            "             BertLayer-70             [-1, 150, 768]               0\n",
            "         BertAttention-71             [-1, 150, 768]               0\n",
            "     BertSelfAttention-72             [-1, 150, 768]               0\n",
            "                Linear-73             [-1, 150, 768]         590,592\n",
            "                Linear-74             [-1, 150, 768]         590,592\n",
            "                Linear-75             [-1, 150, 768]         590,592\n",
            "               Dropout-76         [-1, 12, 150, 150]               0\n",
            "        BertSelfOutput-77             [-1, 150, 768]               0\n",
            "                Linear-78             [-1, 150, 768]         590,592\n",
            "               Dropout-79             [-1, 150, 768]               0\n",
            "             LayerNorm-80             [-1, 150, 768]           1,536\n",
            "      BertIntermediate-81             [-1, 150, 768]               0\n",
            "                Linear-82             [-1, 150, 768]       2,362,368\n",
            "        GELUActivation-83            [-1, 150, 3072]               0\n",
            "            BertOutput-84            [-1, 150, 3072]               0\n",
            "                Linear-85            [-1, 150, 3072]       2,360,064\n",
            "               Dropout-86             [-1, 150, 768]               0\n",
            "             LayerNorm-87             [-1, 150, 768]           1,536\n",
            "             BertLayer-88             [-1, 150, 768]               0\n",
            "         BertAttention-89             [-1, 150, 768]               0\n",
            "     BertSelfAttention-90             [-1, 150, 768]               0\n",
            "                Linear-91             [-1, 150, 768]         590,592\n",
            "                Linear-92             [-1, 150, 768]         590,592\n",
            "                Linear-93             [-1, 150, 768]         590,592\n",
            "               Dropout-94         [-1, 12, 150, 150]               0\n",
            "        BertSelfOutput-95             [-1, 150, 768]               0\n",
            "                Linear-96             [-1, 150, 768]         590,592\n",
            "               Dropout-97             [-1, 150, 768]               0\n",
            "             LayerNorm-98             [-1, 150, 768]           1,536\n",
            "      BertIntermediate-99             [-1, 150, 768]               0\n",
            "               Linear-100             [-1, 150, 768]       2,362,368\n",
            "       GELUActivation-101            [-1, 150, 3072]               0\n",
            "           BertOutput-102            [-1, 150, 3072]               0\n",
            "               Linear-103            [-1, 150, 3072]       2,360,064\n",
            "              Dropout-104             [-1, 150, 768]               0\n",
            "            LayerNorm-105             [-1, 150, 768]           1,536\n",
            "            BertLayer-106             [-1, 150, 768]               0\n",
            "        BertAttention-107             [-1, 150, 768]               0\n",
            "    BertSelfAttention-108             [-1, 150, 768]               0\n",
            "               Linear-109             [-1, 150, 768]         590,592\n",
            "               Linear-110             [-1, 150, 768]         590,592\n",
            "               Linear-111             [-1, 150, 768]         590,592\n",
            "              Dropout-112         [-1, 12, 150, 150]               0\n",
            "       BertSelfOutput-113             [-1, 150, 768]               0\n",
            "               Linear-114             [-1, 150, 768]         590,592\n",
            "              Dropout-115             [-1, 150, 768]               0\n",
            "            LayerNorm-116             [-1, 150, 768]           1,536\n",
            "     BertIntermediate-117             [-1, 150, 768]               0\n",
            "               Linear-118             [-1, 150, 768]       2,362,368\n",
            "       GELUActivation-119            [-1, 150, 3072]               0\n",
            "           BertOutput-120            [-1, 150, 3072]               0\n",
            "               Linear-121            [-1, 150, 3072]       2,360,064\n",
            "              Dropout-122             [-1, 150, 768]               0\n",
            "            LayerNorm-123             [-1, 150, 768]           1,536\n",
            "            BertLayer-124             [-1, 150, 768]               0\n",
            "        BertAttention-125             [-1, 150, 768]               0\n",
            "    BertSelfAttention-126             [-1, 150, 768]               0\n",
            "               Linear-127             [-1, 150, 768]         590,592\n",
            "               Linear-128             [-1, 150, 768]         590,592\n",
            "               Linear-129             [-1, 150, 768]         590,592\n",
            "              Dropout-130         [-1, 12, 150, 150]               0\n",
            "       BertSelfOutput-131             [-1, 150, 768]               0\n",
            "               Linear-132             [-1, 150, 768]         590,592\n",
            "              Dropout-133             [-1, 150, 768]               0\n",
            "            LayerNorm-134             [-1, 150, 768]           1,536\n",
            "     BertIntermediate-135             [-1, 150, 768]               0\n",
            "               Linear-136             [-1, 150, 768]       2,362,368\n",
            "       GELUActivation-137            [-1, 150, 3072]               0\n",
            "           BertOutput-138            [-1, 150, 3072]               0\n",
            "               Linear-139            [-1, 150, 3072]       2,360,064\n",
            "              Dropout-140             [-1, 150, 768]               0\n",
            "            LayerNorm-141             [-1, 150, 768]           1,536\n",
            "            BertLayer-142             [-1, 150, 768]               0\n",
            "        BertAttention-143             [-1, 150, 768]               0\n",
            "    BertSelfAttention-144             [-1, 150, 768]               0\n",
            "               Linear-145             [-1, 150, 768]         590,592\n",
            "               Linear-146             [-1, 150, 768]         590,592\n",
            "               Linear-147             [-1, 150, 768]         590,592\n",
            "              Dropout-148         [-1, 12, 150, 150]               0\n",
            "       BertSelfOutput-149             [-1, 150, 768]               0\n",
            "               Linear-150             [-1, 150, 768]         590,592\n",
            "              Dropout-151             [-1, 150, 768]               0\n",
            "            LayerNorm-152             [-1, 150, 768]           1,536\n",
            "     BertIntermediate-153             [-1, 150, 768]               0\n",
            "               Linear-154             [-1, 150, 768]       2,362,368\n",
            "       GELUActivation-155            [-1, 150, 3072]               0\n",
            "           BertOutput-156            [-1, 150, 3072]               0\n",
            "               Linear-157            [-1, 150, 3072]       2,360,064\n",
            "              Dropout-158             [-1, 150, 768]               0\n",
            "            LayerNorm-159             [-1, 150, 768]           1,536\n",
            "            BertLayer-160             [-1, 150, 768]               0\n",
            "        BertAttention-161             [-1, 150, 768]               0\n",
            "    BertSelfAttention-162             [-1, 150, 768]               0\n",
            "               Linear-163             [-1, 150, 768]         590,592\n",
            "               Linear-164             [-1, 150, 768]         590,592\n",
            "               Linear-165             [-1, 150, 768]         590,592\n",
            "              Dropout-166         [-1, 12, 150, 150]               0\n",
            "       BertSelfOutput-167             [-1, 150, 768]               0\n",
            "               Linear-168             [-1, 150, 768]         590,592\n",
            "              Dropout-169             [-1, 150, 768]               0\n",
            "            LayerNorm-170             [-1, 150, 768]           1,536\n",
            "     BertIntermediate-171             [-1, 150, 768]               0\n",
            "               Linear-172             [-1, 150, 768]       2,362,368\n",
            "       GELUActivation-173            [-1, 150, 3072]               0\n",
            "           BertOutput-174            [-1, 150, 3072]               0\n",
            "               Linear-175            [-1, 150, 3072]       2,360,064\n",
            "              Dropout-176             [-1, 150, 768]               0\n",
            "            LayerNorm-177             [-1, 150, 768]           1,536\n",
            "            BertLayer-178             [-1, 150, 768]               0\n",
            "        BertAttention-179             [-1, 150, 768]               0\n",
            "    BertSelfAttention-180             [-1, 150, 768]               0\n",
            "               Linear-181             [-1, 150, 768]         590,592\n",
            "               Linear-182             [-1, 150, 768]         590,592\n",
            "               Linear-183             [-1, 150, 768]         590,592\n",
            "              Dropout-184         [-1, 12, 150, 150]               0\n",
            "       BertSelfOutput-185             [-1, 150, 768]               0\n",
            "               Linear-186             [-1, 150, 768]         590,592\n",
            "              Dropout-187             [-1, 150, 768]               0\n",
            "            LayerNorm-188             [-1, 150, 768]           1,536\n",
            "     BertIntermediate-189             [-1, 150, 768]               0\n",
            "               Linear-190             [-1, 150, 768]       2,362,368\n",
            "       GELUActivation-191            [-1, 150, 3072]               0\n",
            "           BertOutput-192            [-1, 150, 3072]               0\n",
            "               Linear-193            [-1, 150, 3072]       2,360,064\n",
            "              Dropout-194             [-1, 150, 768]               0\n",
            "            LayerNorm-195             [-1, 150, 768]           1,536\n",
            "            BertLayer-196             [-1, 150, 768]               0\n",
            "        BertAttention-197             [-1, 150, 768]               0\n",
            "    BertSelfAttention-198             [-1, 150, 768]               0\n",
            "               Linear-199             [-1, 150, 768]         590,592\n",
            "               Linear-200             [-1, 150, 768]         590,592\n",
            "               Linear-201             [-1, 150, 768]         590,592\n",
            "              Dropout-202         [-1, 12, 150, 150]               0\n",
            "       BertSelfOutput-203             [-1, 150, 768]               0\n",
            "               Linear-204             [-1, 150, 768]         590,592\n",
            "              Dropout-205             [-1, 150, 768]               0\n",
            "            LayerNorm-206             [-1, 150, 768]           1,536\n",
            "     BertIntermediate-207             [-1, 150, 768]               0\n",
            "               Linear-208             [-1, 150, 768]       2,362,368\n",
            "       GELUActivation-209            [-1, 150, 3072]               0\n",
            "           BertOutput-210            [-1, 150, 3072]               0\n",
            "               Linear-211            [-1, 150, 3072]       2,360,064\n",
            "              Dropout-212             [-1, 150, 768]               0\n",
            "            LayerNorm-213             [-1, 150, 768]           1,536\n",
            "            BertLayer-214             [-1, 150, 768]               0\n",
            "        BertAttention-215             [-1, 150, 768]               0\n",
            "    BertSelfAttention-216             [-1, 150, 768]               0\n",
            "               Linear-217             [-1, 150, 768]         590,592\n",
            "               Linear-218             [-1, 150, 768]         590,592\n",
            "               Linear-219             [-1, 150, 768]         590,592\n",
            "              Dropout-220         [-1, 12, 150, 150]               0\n",
            "       BertSelfOutput-221             [-1, 150, 768]               0\n",
            "               Linear-222             [-1, 150, 768]         590,592\n",
            "              Dropout-223             [-1, 150, 768]               0\n",
            "            LayerNorm-224             [-1, 150, 768]           1,536\n",
            "     BertIntermediate-225             [-1, 150, 768]               0\n",
            "               Linear-226             [-1, 150, 768]       2,362,368\n",
            "       GELUActivation-227            [-1, 150, 3072]               0\n",
            "           BertOutput-228            [-1, 150, 3072]               0\n",
            "               Linear-229            [-1, 150, 3072]       2,360,064\n",
            "              Dropout-230             [-1, 150, 768]               0\n",
            "            LayerNorm-231             [-1, 150, 768]           1,536\n",
            "           BertPooler-232             [-1, 150, 768]               0\n",
            "               Linear-233                  [-1, 768]         590,592\n",
            "                 Tanh-234                  [-1, 768]               0\n",
            "       Conv1DEmbedder-235             [-1, 150, 768]               0\n",
            "               Conv1d-236             [-1, 768, 150]         393,728\n",
            "               Conv1d-237             [-1, 512, 150]         131,328\n",
            "               Conv1d-238             [-1, 256, 150]          32,896\n",
            "               Conv1d-239             [-1, 128, 150]           8,256\n",
            "   PositionalEncoding-240              [-1, 150, 64]               0\n",
            "              Dropout-241              [-1, 150, 64]               0\n",
            "          Transformer-242                         ()               0\n",
            "   TransformerEncoder-243              [-1, 259, 64]               0\n",
            "TransformerEncoderLayer-244              [-1, 259, 64]               0\n",
            "   MultiheadAttention-245              [-1, 259, 64]               0\n",
            "              Dropout-246              [-1, 259, 64]               0\n",
            "            LayerNorm-247              [-1, 259, 64]             128\n",
            "               Linear-248              [-1, 259, 64]         133,120\n",
            "              Dropout-249            [-1, 259, 2048]               0\n",
            "               Linear-250            [-1, 259, 2048]         131,136\n",
            "              Dropout-251              [-1, 259, 64]               0\n",
            "            LayerNorm-252              [-1, 259, 64]             128\n",
            "TransformerEncoderLayer-253              [-1, 259, 64]               0\n",
            "   MultiheadAttention-254              [-1, 259, 64]               0\n",
            "              Dropout-255              [-1, 259, 64]               0\n",
            "            LayerNorm-256              [-1, 259, 64]             128\n",
            "               Linear-257              [-1, 259, 64]         133,120\n",
            "              Dropout-258            [-1, 259, 2048]               0\n",
            "               Linear-259            [-1, 259, 2048]         131,136\n",
            "              Dropout-260              [-1, 259, 64]               0\n",
            "            LayerNorm-261              [-1, 259, 64]             128\n",
            "TransformerEncoderLayer-262              [-1, 259, 64]               0\n",
            "   MultiheadAttention-263              [-1, 259, 64]               0\n",
            "              Dropout-264              [-1, 259, 64]               0\n",
            "            LayerNorm-265              [-1, 259, 64]             128\n",
            "               Linear-266              [-1, 259, 64]         133,120\n",
            "              Dropout-267            [-1, 259, 2048]               0\n",
            "               Linear-268            [-1, 259, 2048]         131,136\n",
            "              Dropout-269              [-1, 259, 64]               0\n",
            "            LayerNorm-270              [-1, 259, 64]             128\n",
            "TransformerEncoderLayer-271              [-1, 259, 64]               0\n",
            "   MultiheadAttention-272              [-1, 259, 64]               0\n",
            "              Dropout-273              [-1, 259, 64]               0\n",
            "            LayerNorm-274              [-1, 259, 64]             128\n",
            "               Linear-275              [-1, 259, 64]         133,120\n",
            "              Dropout-276            [-1, 259, 2048]               0\n",
            "               Linear-277            [-1, 259, 2048]         131,136\n",
            "              Dropout-278              [-1, 259, 64]               0\n",
            "            LayerNorm-279              [-1, 259, 64]             128\n",
            "TransformerEncoderLayer-280              [-1, 259, 64]               0\n",
            "   MultiheadAttention-281              [-1, 259, 64]               0\n",
            "              Dropout-282              [-1, 259, 64]               0\n",
            "            LayerNorm-283              [-1, 259, 64]             128\n",
            "               Linear-284              [-1, 259, 64]         133,120\n",
            "              Dropout-285            [-1, 259, 2048]               0\n",
            "               Linear-286            [-1, 259, 2048]         131,136\n",
            "              Dropout-287              [-1, 259, 64]               0\n",
            "            LayerNorm-288              [-1, 259, 64]             128\n",
            "TransformerEncoderLayer-289              [-1, 259, 64]               0\n",
            "   MultiheadAttention-290              [-1, 259, 64]               0\n",
            "              Dropout-291              [-1, 259, 64]               0\n",
            "            LayerNorm-292              [-1, 259, 64]             128\n",
            "               Linear-293              [-1, 259, 64]         133,120\n",
            "              Dropout-294            [-1, 259, 2048]               0\n",
            "               Linear-295            [-1, 259, 2048]         131,136\n",
            "              Dropout-296              [-1, 259, 64]               0\n",
            "            LayerNorm-297              [-1, 259, 64]             128\n",
            "            LayerNorm-298              [-1, 259, 64]             128\n",
            "   TransformerDecoder-299              [-1, 150, 64]               0\n",
            "TransformerDecoderLayer-300              [-1, 150, 64]               0\n",
            "   MultiheadAttention-301              [-1, 150, 64]               0\n",
            "              Dropout-302              [-1, 150, 64]               0\n",
            "            LayerNorm-303              [-1, 150, 64]             128\n",
            "   MultiheadAttention-304              [-1, 150, 64]               0\n",
            "              Dropout-305              [-1, 150, 64]               0\n",
            "            LayerNorm-306              [-1, 150, 64]             128\n",
            "               Linear-307              [-1, 150, 64]         133,120\n",
            "              Dropout-308            [-1, 150, 2048]               0\n",
            "               Linear-309            [-1, 150, 2048]         131,136\n",
            "              Dropout-310              [-1, 150, 64]               0\n",
            "            LayerNorm-311              [-1, 150, 64]             128\n",
            "TransformerDecoderLayer-312              [-1, 150, 64]               0\n",
            "   MultiheadAttention-313              [-1, 150, 64]               0\n",
            "              Dropout-314              [-1, 150, 64]               0\n",
            "            LayerNorm-315              [-1, 150, 64]             128\n",
            "   MultiheadAttention-316              [-1, 150, 64]               0\n",
            "              Dropout-317              [-1, 150, 64]               0\n",
            "            LayerNorm-318              [-1, 150, 64]             128\n",
            "               Linear-319              [-1, 150, 64]         133,120\n",
            "              Dropout-320            [-1, 150, 2048]               0\n",
            "               Linear-321            [-1, 150, 2048]         131,136\n",
            "              Dropout-322              [-1, 150, 64]               0\n",
            "            LayerNorm-323              [-1, 150, 64]             128\n",
            "TransformerDecoderLayer-324              [-1, 150, 64]               0\n",
            "   MultiheadAttention-325              [-1, 150, 64]               0\n",
            "              Dropout-326              [-1, 150, 64]               0\n",
            "            LayerNorm-327              [-1, 150, 64]             128\n",
            "   MultiheadAttention-328              [-1, 150, 64]               0\n",
            "              Dropout-329              [-1, 150, 64]               0\n",
            "            LayerNorm-330              [-1, 150, 64]             128\n",
            "               Linear-331              [-1, 150, 64]         133,120\n",
            "              Dropout-332            [-1, 150, 2048]               0\n",
            "               Linear-333            [-1, 150, 2048]         131,136\n",
            "              Dropout-334              [-1, 150, 64]               0\n",
            "            LayerNorm-335              [-1, 150, 64]             128\n",
            "TransformerDecoderLayer-336              [-1, 150, 64]               0\n",
            "   MultiheadAttention-337              [-1, 150, 64]               0\n",
            "              Dropout-338              [-1, 150, 64]               0\n",
            "            LayerNorm-339              [-1, 150, 64]             128\n",
            "   MultiheadAttention-340              [-1, 150, 64]               0\n",
            "              Dropout-341              [-1, 150, 64]               0\n",
            "            LayerNorm-342              [-1, 150, 64]             128\n",
            "               Linear-343              [-1, 150, 64]         133,120\n",
            "              Dropout-344            [-1, 150, 2048]               0\n",
            "               Linear-345            [-1, 150, 2048]         131,136\n",
            "              Dropout-346              [-1, 150, 64]               0\n",
            "            LayerNorm-347              [-1, 150, 64]             128\n",
            "TransformerDecoderLayer-348              [-1, 150, 64]               0\n",
            "   MultiheadAttention-349              [-1, 150, 64]               0\n",
            "              Dropout-350              [-1, 150, 64]               0\n",
            "            LayerNorm-351              [-1, 150, 64]             128\n",
            "   MultiheadAttention-352              [-1, 150, 64]               0\n",
            "              Dropout-353              [-1, 150, 64]               0\n",
            "            LayerNorm-354              [-1, 150, 64]             128\n",
            "               Linear-355              [-1, 150, 64]         133,120\n",
            "              Dropout-356            [-1, 150, 2048]               0\n",
            "               Linear-357            [-1, 150, 2048]         131,136\n",
            "              Dropout-358              [-1, 150, 64]               0\n",
            "            LayerNorm-359              [-1, 150, 64]             128\n",
            "TransformerDecoderLayer-360              [-1, 150, 64]               0\n",
            "   MultiheadAttention-361              [-1, 150, 64]               0\n",
            "              Dropout-362              [-1, 150, 64]               0\n",
            "            LayerNorm-363              [-1, 150, 64]             128\n",
            "   MultiheadAttention-364              [-1, 150, 64]               0\n",
            "              Dropout-365              [-1, 150, 64]               0\n",
            "            LayerNorm-366              [-1, 150, 64]             128\n",
            "               Linear-367              [-1, 150, 64]         133,120\n",
            "              Dropout-368            [-1, 150, 2048]               0\n",
            "               Linear-369            [-1, 150, 2048]         131,136\n",
            "              Dropout-370              [-1, 150, 64]               0\n",
            "            LayerNorm-371              [-1, 150, 64]             128\n",
            "            LayerNorm-372              [-1, 150, 64]             128\n",
            "               Linear-373              [-1, 150, 64]       1,950,000\n",
            "=======================================================================\n",
            "Total params: 115,779,760\n",
            "Trainable params: 115,779,760\n",
            "Non-trainable params: 0\n",
            "-----------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from modelsummary import summary\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "BATCH_SIZE_TEST = 1\n",
        "\n",
        "\n",
        "src = torch.randn(BATCH_SIZE_TEST, MAX_FRAMES, IN_FEATURES).to(DEVICE)\n",
        "tgt = torch.randint(0, tokenizer.vocab_size, (BATCH_SIZE_TEST, MAX_TOKENS)).to(DEVICE)\n",
        "tgt_mask = torch.zeros(MAX_TOKENS, MAX_TOKENS).to(DEVICE)\n",
        "tgt_padding_mask = torch.randint(0, 2, (BATCH_SIZE_TEST, MAX_TOKENS)).bool().to(DEVICE)\n",
        "print(src.shape, tgt.shape, tgt_mask.shape, tgt_padding_mask.shape)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "summary(model, src, tgt, tgt_mask, tgt_padding_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUdekzqZXbkG"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DS46dluKtnOd"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "\t!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_he4uDwy7Dwu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
          ]
        }
      ],
      "source": [
        "from torch import Tensor\n",
        "from torch.optim import Adam\n",
        "from torch.nn.functional import cross_entropy\n",
        "from torchmetrics import Accuracy\n",
        "import lightning as L\n",
        "\n",
        "\n",
        "class LKeypointsTransformer(L.LightningModule):\n",
        "\n",
        "    def __init__(self, model: KeypointsTransformer, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.loss_fn = cross_entropy\n",
        "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes, ignore_index=PAD_IDX)\n",
        "        self.save_hyperparameters(ignore=['model'])\n",
        "\n",
        "    def forward(self, src: Tensor, tgt: Tensor, tgt_mask: Tensor, tgt_padding_mask: Tensor):\n",
        "        return self.model(src, tgt, tgt_mask, tgt_padding_mask)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def run_on_batch(self, batch):\n",
        "        src, tgt = batch\n",
        "        # tgt_input and tgt_ouptut are displaced by one position, so tgt_input[i] is the input to the model and tgt_output[i] is the expected output\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_mask, tgt_padding_mask = create_target_mask(tgt_input, PAD_IDX, DEVICE)\n",
        "        logits = self.model(src, tgt_input, tgt_mask, tgt_padding_mask)\n",
        "        tgt_output = tgt[:, 1:]\n",
        "        loss = self.loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_output.reshape(-1), ignore_index=PAD_IDX)\n",
        "        accuracy = self.accuracy(logits.reshape(-1, logits.shape[-1]), tgt_output.reshape(-1))\n",
        "        return loss, accuracy\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, accuracy = self.run_on_batch(batch)\n",
        "        self.log(\"train_loss\", loss, on_epoch=True)\n",
        "        self.log(\"train_accuracy\", accuracy, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, accuracy = self.run_on_batch(batch)\n",
        "        self.log(\"val_loss\", loss, on_epoch=True)\n",
        "        self.log(\"val_accuracy\", accuracy, on_epoch=True, batch_size=len(batch))\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, accuracy = self.run_on_batch(batch)\n",
        "        self.log(\"test_loss\", loss)\n",
        "        self.log(\"test_accuracy\", accuracy)\n",
        "        return loss, accuracy\n",
        "\n",
        "    # function to generate output sequence using greedy algorithm\n",
        "    def greedy_translate(self, src):\n",
        "        memory = self.model.encode(src)\n",
        "        ys = torch.ones(1, 1).fill_(BOS_IDX).to(DEVICE)\n",
        "        for i in range(MAX_TOKENS-1):\n",
        "            tgt_mask = generate_square_subsequent_mask(ys.size(1), DEVICE)\n",
        "            out = model.decode(ys, memory, tgt_mask)\n",
        "            prob = model.generator(out[:, -1])\n",
        "            _, next_word = torch.max(prob, dim=1)\n",
        "            next_word = next_word.item()\n",
        "            # print(next_word)\n",
        "            ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "            if next_word == EOS_IDX:\n",
        "                break\n",
        "        return tokenizer.decode([int(x) for x in ys[0].tolist()], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "l_model = LKeypointsTransformer(model, tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Av5r8Xx5YoQE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1172\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1171\u001b[0m wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[0;32m-> 1172\u001b[0m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m wi\u001b[38;5;241m.\u001b[39msettings\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:189\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m setup_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_service\u001b[39m\u001b[38;5;124m\"\u001b[39m: _disable_service}\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl \u001b[38;5;241m=\u001b[39m \u001b[43mwandb_setup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetup_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Make sure we have a logger setup (might be an early logger)\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/wandb/sdk/wandb_setup.py:327\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(settings)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\n\u001b[1;32m    325\u001b[0m     settings: Optional[Settings] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    326\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_WandbSetup\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 327\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/wandb/sdk/wandb_setup.py:320\u001b[0m, in \u001b[0;36m_setup\u001b[0;34m(settings, _reset)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m wl \u001b[38;5;241m=\u001b[39m \u001b[43m_WandbSetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wl\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/wandb/sdk/wandb_setup.py:303\u001b[0m, in \u001b[0;36m_WandbSetup.__init__\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m _WandbSetup\u001b[38;5;241m.\u001b[39m_instance \u001b[38;5;241m=\u001b[39m \u001b[43m_WandbSetup__WandbSetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/wandb/sdk/wandb_setup.py:108\u001b[0m, in \u001b[0;36m_WandbSetup__WandbSetup.__init__\u001b[0;34m(self, pid, settings, environ)\u001b[0m\n\u001b[1;32m    106\u001b[0m _set_logger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_logger)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_early_logger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# self._settings.freeze()\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/wandb/sdk/wandb_setup.py:136\u001b[0m, in \u001b[0;36m_WandbSetup__WandbSetup._settings_setup\u001b[0;34m(self, settings, early_logger)\u001b[0m\n\u001b[1;32m    134\u001b[0m     s\u001b[38;5;241m.\u001b[39m_apply_setup(settings, _logger\u001b[38;5;241m=\u001b[39mearly_logger)\n\u001b[0;32m--> 136\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_settings_from_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s\u001b[38;5;241m.\u001b[39m_cli_only_mode:\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/wandb/sdk/wandb_settings.py:1763\u001b[0m, in \u001b[0;36mSettings._infer_settings_from_environment\u001b[0;34m(self, _logger)\u001b[0m\n\u001b[1;32m   1762\u001b[0m     settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_args\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m-> 1763\u001b[0m settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_os\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mplatform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplatform\u001b[49m\u001b[43m(\u001b[49m\u001b[43maliased\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_python\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m platform\u001b[38;5;241m.\u001b[39mpython_version()\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/platform.py:1259\u001b[0m, in \u001b[0;36mplatform\u001b[0;34m(aliased, terse)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1259\u001b[0m     bits, linkage \u001b[38;5;241m=\u001b[39m \u001b[43marchitecture\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1260\u001b[0m     platform \u001b[38;5;241m=\u001b[39m _platform(system, release, machine,\n\u001b[1;32m   1261\u001b[0m                          processor, bits, linkage)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/platform.py:681\u001b[0m, in \u001b[0;36marchitecture\u001b[0;34m(executable, bits, linkage)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executable:\n\u001b[0;32m--> 681\u001b[0m     fileout \u001b[38;5;241m=\u001b[39m \u001b[43m_syscmd_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/platform.py:630\u001b[0m, in \u001b[0;36m_syscmd_file\u001b[0;34m(target, default)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# -b: do not prepend filenames to output lines (brief mode)\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVNULL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError):\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/subprocess.py:466\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m empty\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/subprocess.py:1883\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1882\u001b[0m fds_to_keep\u001b[38;5;241m.\u001b[39madd(errpipe_write)\n\u001b[0;32m-> 1883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[43m_fork_exec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfds_to_keep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrpipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrpipe_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_USE_VFORK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_child_created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WandbLogger\n\u001b[1;32m      6\u001b[0m wandb_logger \u001b[38;5;241m=\u001b[39m WandbLogger(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrwth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mwandb_logger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m      8\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBATCH_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m: BATCH_SIZE,\n\u001b[1;32m      9\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEVICE\u001b[39m\u001b[38;5;124m\"\u001b[39m: DEVICE,\n\u001b[1;32m     10\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAX_FRAMES\u001b[39m\u001b[38;5;124m\"\u001b[39m: MAX_FRAMES,\n\u001b[1;32m     11\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAX_TOKENS\u001b[39m\u001b[38;5;124m\"\u001b[39m: MAX_TOKENS,\n\u001b[1;32m     12\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEXT_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m: TEXT_MODEL,\n\u001b[1;32m     13\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKEYS_EMB_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m: KEYS_EMB_SIZE,\n\u001b[1;32m     14\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m: D_MODEL,\n\u001b[1;32m     15\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKERNEL_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m: KERNEL_SIZE,\n\u001b[1;32m     16\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROPOUT\u001b[39m\u001b[38;5;124m\"\u001b[39m: DROPOUT,\n\u001b[1;32m     17\u001b[0m })\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     20\u001b[0m     logger\u001b[38;5;241m=\u001b[39mwandb_logger,\n\u001b[1;32m     21\u001b[0m     default_root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \t\t),],\n\u001b[1;32m     31\u001b[0m )\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/lightning/fabric/loggers/logger.py:118\u001b[0m, in \u001b[0;36mrank_zero_experiment.<locals>.experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank_zero_only\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _DummyExperiment()\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:401\u001b[0m, in \u001b[0;36mWandbLogger.experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39m_attach(attach_id)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# create new wandb process\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wandb_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# define default x-axis\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment, (Run, RunDisabled)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefine_metric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     ):\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/slt_models_tryout/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1197\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logger\n\u001b[1;32m   1198\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "\n",
        "PRECISION = 32\n",
        "\n",
        "wandb_logger = WandbLogger(project=\"rwth\")\n",
        "wandb_logger.experiment.config.update({\n",
        "\t# System hyperparameters\n",
        "\t\"DEVICE\": DEVICE,\n",
        "\t\"PRECISION\": PRECISION,\n",
        "\t# Data hyperparameters\n",
        "\t\"BATCH_SIZE\": BATCH_SIZE,\n",
        "\t\"MAX_FRAMES\": MAX_FRAMES,\n",
        "\t\"MAX_TOKENS\": MAX_TOKENS,\n",
        "\t\"TEXT_MODEL\": TEXT_MODEL,\n",
        "\t# Model hyperparameters\n",
        "\t\"D_MODEL\": D_MODEL,\n",
        "\t\"DROPOUT\": DROPOUT,\n",
        "\t\"EMBEDDINGS\": \"BERT\"\n",
        "})\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "\tmonitor='val_loss',\n",
        "\tdirpath='checkpoints/',\n",
        "\tfilename=f'rwth-{wandb_logger.experiment.name}-{{epoch:02d}}-{{step:02d}}-{{val_loss:.2f}}',\n",
        "\tmode='min',\n",
        "\tsave_last=True\n",
        ")\n",
        "checkpoint_callback.CHECKPOINT_NAME_LAST = f\"rwth-{wandb_logger.experiment.name}-last.ckpt\"\n",
        "\n",
        "trainer = L.Trainer(\n",
        "    logger=wandb_logger,\n",
        "    default_root_dir=\"./checkpoint\",\n",
        "\t\tprecision=PRECISION,\n",
        "    callbacks=[\n",
        "\t\tEarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5),\n",
        "\t\tcheckpoint_callback,],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU8ozZtpRaX4"
      },
      "outputs": [],
      "source": [
        "trainer.fit(\n",
        "    model=l_model,\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=test_loader,\n",
        "    # ckpt_path=\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ibgAbSqifv1"
      },
      "outputs": [],
      "source": [
        "# CHKP = \"/content/checkpoints/\"\n",
        "# l_model = LKeypointsTransformer.load_from_checkpoint(CHKP, model=model, num_classes=tokenizer.vocab_size)\n",
        "\n",
        "\n",
        "trainer.test(\n",
        "    model=l_model,\n",
        "\tdataloaders=test_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "swGo1wYgRaX4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 27, sample 0\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'l_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# adds extra dimension representing the batch\u001b[39;00m\n\u001b[1;32m     20\u001b[0m src_0 \u001b[38;5;241m=\u001b[39m src[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m l_model \u001b[38;5;241m=\u001b[39m \u001b[43ml_model\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     22\u001b[0m pred \u001b[38;5;241m=\u001b[39m (l_model\u001b[38;5;241m.\u001b[39mgreedy_translate(src_0))\n\u001b[1;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode([\u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tgt[i]\u001b[38;5;241m.\u001b[39mtolist()], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'l_model' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from torchmetrics.functional.text import bleu_score\n",
        "\n",
        "\n",
        "results = []\n",
        "l_model = l_model.to(DEVICE)\n",
        "\n",
        "for batch_idx, (src, tgt) in enumerate(test_loader):\n",
        "\t\tsrc = src.to(DEVICE)\n",
        "\t\tfor i in range(len(src)):\n",
        "\t\t\t\tprint(f\"Batch {batch_idx}, sample {i}\")\n",
        "\t\t\t\t# adds extra dimension representing the batch\n",
        "\t\t\t\tsrc_0 = src[i].unsqueeze(0)\n",
        "\t\t\t\tpred = (l_model.greedy_translate(src_0))\n",
        "\t\t\t\ty = tokenizer.decode([int(x) for x in tgt[i].tolist()], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\t\t\t\tresults.append((pred, y) + tuple(bleu_score(pred, [y], n_gram=n).item() for n in range(1, 5)))\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=[\"pred\", \"tgt\", \"bleu_1\", \"bleu_2\", \"bleu_3\", \"bleu_4\"])\n",
        "results_df.to_csv(f\"results-{wandb_logger.experiment.name}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HWS3364RaX-"
      },
      "outputs": [],
      "source": [
        "results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AIragUlzRaX_"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bleu_1</th>\n",
              "      <th>bleu_2</th>\n",
              "      <th>bleu_3</th>\n",
              "      <th>bleu_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>642.000000</td>\n",
              "      <td>642.000000</td>\n",
              "      <td>642.000000</td>\n",
              "      <td>642.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.109511</td>\n",
              "      <td>0.033171</td>\n",
              "      <td>0.018060</td>\n",
              "      <td>0.011550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.125266</td>\n",
              "      <td>0.102343</td>\n",
              "      <td>0.086480</td>\n",
              "      <td>0.076505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.026352</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.148773</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.711767</td>\n",
              "      <td>0.706184</td>\n",
              "      <td>0.699509</td>\n",
              "      <td>0.691309</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           bleu_1      bleu_2      bleu_3      bleu_4\n",
              "count  642.000000  642.000000  642.000000  642.000000\n",
              "mean     0.109511    0.033171    0.018060    0.011550\n",
              "std      0.125266    0.102343    0.086480    0.076505\n",
              "min      0.000000    0.000000    0.000000    0.000000\n",
              "25%      0.026352    0.000000    0.000000    0.000000\n",
              "50%      0.076923    0.000000    0.000000    0.000000\n",
              "75%      0.148773    0.000000    0.000000    0.000000\n",
              "max      0.711767    0.706184    0.699509    0.691309"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df.describe()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
